---
author: "Certseeds"
date: "2025-08-21"
lastmod: "2025-08-21"
title: "需求来自体验工作流程"
description: "vitebooks都做了些什么"
tags: ["notes", "experience"]
---

# 需求来自体验工作流程

> 上文: <https://blog.certseeds.com/2024/vitebooks_and_depage.html>

2025年vitebooks项目主工程基本完工, 现在来点总结

## vitebooks 在做什么?

就 <https://github.com/Certseeds/vitebooks> 项目而言, 最主要的目的是保存 `Horus Heresy` 系列小说的译文, 并使用技术手段对其进行格式化, 归一化等操作以提升阅读效果.

原有逻辑包括章节拆分, 标点替换, 构建关键人名替换表, 章节跳转.

原有计划中的动态人名表以及基于本地LLM(现在还是Qwen2.5:14b)的人名提取都完成了, 还新增了用embdding模型做向量化, 对人名做聚类的功能.

## 脚注提取

原始几个目录中几乎没有脚注, 出现几个之后是手动处理的, 结果越处理越被动, 最后发现不能再手动了, 没有意义, 就用脚本实现了脚注提取功能, 并复用代码做成了单独的网页 <https://pages.certseeds.com/proofreading/footnote.html>, 其实可以放在vitebooks仓库内的.

## 制作epub

刚开始的版本里没有epub, 移动端上只能用网页看, 结果发现虽然能看, 但是网页端翻页需要加载, 并且更要命的是, 网页端无法在发现的错误处打标注印记: 还得找一个第三方来记录, 麻烦, 并且容易遗漏.

为了优化移动端的阅读效果, 就用rust给pandoc写了一层包装, 调用pandoc命令行将md打包成epub, 并将epub统一上传到另外一个pages上, 并为其分配了和主站独立的下载域名. 这样可以在移动平台上借助已有的阅读app方便的阅读, 备注, 更好的优化译文.

## 标点对齐

由于有些译文实在太不关注标点符号的对齐等问题了, 一整本书能有几百上千处引号不对, 人力难以企及(没错, 当整本书几十几百处的时候还真是手动处理的), 就写了用llm对引号不对齐的行进行处理的脚本, 发现使用 `claude-sonnet4` 的API吊打本地模型, 就一直使用API对行进行净化, 通过校验原有文字顺序来保证不会改动原文.

TODO: 使用本地脚本对单引号/双引号进行统一校对, 总感觉之前写的脚本有点问题, 扫出来的可能不太够.

## 元数据增强

做好了依赖管理后, 我发现depage上还可以做更多.

1. 可以做简单的作者排序, 将meta.toml中记录的作者-书映射关系记录出来, 并且按书的数量排个序.
2. 可以给每本书标记 "出场阵营", 令我震惊的是第一名居然是 "钢铁之手", 随后是荷子, 怀言者,  恶魔, 然后才是极限战士. 我还为阵营添加了多选项实时搜索, 可以选择左侧序号, 并submit查询选中阵营同时出现的书.

TODO: 战役关键字支持, 可以考虑为'考斯之战', '萨拉马斯战役' 等战役添加专门的关键字

## markdown提取

有些时候需要参考原始电子书, 或者复制原文, 或者查看格式, 但是使用calibre直接看html的体验确实有点糟糕, 光是查看格式还好, 到了复制原文简直要命. 由于epub本质上是zip(html+部分约定元数据), 只要能将zip解压出来, 就能获取到一章一章的html.

这些html内使用特殊的类名等html标记来实现一些特殊效果(比如大章节内部的小节切换), 摸透之后可以统一协程turndown的规则, 少数几本就能统计出常见的效果.

做好了单章的效果之后, 从复制原文变成了脚本提取, 之后将脚本改造为能直接提取整个目录的文件(因为一个epub内html文件的规则都是相通的), 这极大的加速了原文-译文比对的效果: 现在这两者可以在相同的编辑器里面打开了!

脚本实现后, 可以很方便的使用copilot实现一个原理相同的页面, 还能复用代码, 将核心代码放子目录, 既能供脚本调用, 也能构建出网页. 产物: <https://vitebooks.certseeds.com/turndown/index.html>

TODO: 尝试提取更多的epub, 提取更多规则.

## 使用大模型API进行翻译

完成了markdown提取后我们得到了一整个目录的md文件, 紧接着的就是翻译. 由于只有极少数需要手动翻译的量, 主要采用的是对原文进行预处理, 对译文进行后处理, 并使用prompt稍微控制翻译风格即可.

我在几次尝试后还是认为 `claude/sonnet4` 在这项翻译任务上最能遵守指令, gemini会过多思考消耗太多token(经常能有20倍输入), deepseek别提了, 太有'创造力'了, 无法遵守指令达到预期效果.

这样转换还达到了一个意料之外的结果: 由于turndown得到的原文包括星号和段落分割`--------`横线, 经过一边API之后的产物也天然携带它们, 产出自带格式了! 大大节约了时间

同上使用copilot简单编辑之后得到一个对照式翻译界面: <https://vitebooks.certseeds.com/web-cmp-trans/index.html>

## 使用pnpm workspace来管理monorepo

起初的版本只有一个package.json, 都不需要怎么管理.

之后脚本多了起来, 开始添加依赖, 添加开发依赖, 这时候也很简单.

depage搭建起来之后有两个package.json, 一次更新依赖有些复杂了.

直到写完了三个子页面之后, 现在一个仓库有四个package.json, 一次更新依赖要切换三次目录, 麻烦不说还容易遗漏, 自然而然的问copilot并得出了pnpm workspace的解决办法, 使用catalog完美解决了依赖版本问题.

## 点题

最后点个题, 基本上的开发流程是, 我先手动走通流程, 明确需要进行什么操作, 并且手动维护一段时间直到我被烦到不想做. 然后从单文件级别做起, 到目录级别或者一本书的级别, 最后将脚本翻译到网页上放开用.

如果不走一开始的路, 要干什么都不知道, 怎么写需求, 怎么开发? 如果自己不是产物的第一体验者, 又怎么能构建一个开发-测试循环, 优化产物体验.
